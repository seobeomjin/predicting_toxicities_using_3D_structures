{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/beomjinseo/anaconda3/envs/pafnucy_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import pandas as pd\n",
    "from math import sqrt, ceil\n",
    "\n",
    "import h5py\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfbio.data import Featurizer, make_grid, rotate\n",
    "import net_3 as net ## custom network for predicting ic50\n",
    "\n",
    "import os.path\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "sns.set_color_codes()\n",
    "color = {'training': 'b', 'validation': 'g', 'test': 'r'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'dataset/train_valid_test_data/exp3/'\n",
    "datasets = ['training', 'validation', 'test']\n",
    "\n",
    "batch_size = 20\n",
    "conv_channels=[64, 128, 256]\n",
    "dense_sizes=[1000, 500, 200]\n",
    "conv_patch=5\n",
    "pool_patch=2\n",
    "lmbda=0.001\n",
    "learning_rate=1e-5\n",
    "to_keep = 10\n",
    "\n",
    "splitted_datasets = ['training1', 'training2','validation1','validation2', 'test1','test2']\n",
    "protein_list = ['andro','estro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- FEATURES ----\n",
      "\n",
      "atomic properties: ['B', 'C', 'N', 'O', 'P', 'S', 'Se', 'halogen', 'metal', 'hyb', 'heavyvalence', 'heterovalence', 'partialcharge', 'molcode', 'hydrophobic', 'aromatic', 'acceptor', 'donor', 'ring']\n"
     ]
    }
   ],
   "source": [
    "featurizer = Featurizer()\n",
    "\n",
    "print('\\n---- FEATURES ----\\n')\n",
    "print('atomic properties:', featurizer.FEATURE_NAMES)\n",
    "\n",
    "columns = {name: i for i, name in enumerate(featurizer.FEATURE_NAMES)}\n",
    "\n",
    "ids = {}\n",
    "toxicity = {}\n",
    "coords = {}\n",
    "features = {}\n",
    "\n",
    "splitted_ids = {}\n",
    "splitted_toxicity = {}\n",
    "splitted_coords = {}\n",
    "splitted_features = {}\n",
    "\n",
    "for dictionary in [ids, toxicity, coords, features]:\n",
    "    for dataset_name in datasets:\n",
    "        dictionary[dataset_name] = []\n",
    "        \n",
    "for dictionary in [splitted_ids, splitted_toxicity, splitted_coords, splitted_features]:\n",
    "    for splitted_dataset_name in splitted_datasets:\n",
    "        dictionary[splitted_dataset_name] = []\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset_path = os.path.join(input_dir, '%s_set.hdf' % dataset_name)\n",
    "    with h5py.File(dataset_path, 'r') as f:\n",
    "        for pdb_id in f: #pdb_id  >>> androgenSDF0\n",
    "            dataset = f[pdb_id]\n",
    "            for i in range(len(protein_list)):\n",
    "                if protein_list[i] in pdb_id : \n",
    "                    splitted_coords[dataset_name + str(i+1)].append(dataset[:, :3])\n",
    "                    splitted_features[dataset_name + str(i+1)].append(dataset[:, 3:])\n",
    "                    splitted_toxicity[dataset_name + str(i+1)].append(dataset.attrs['toxicity'])\n",
    "                    splitted_ids[dataset_name + str(i+1)].append(pdb_id) \n",
    "    \n",
    "for k in splitted_ids.keys():\n",
    "    splitted_ids[k] = np.array(splitted_ids[k])\n",
    "    splitted_toxicity[k] = np.reshape(splitted_toxicity[k], (-1, 1))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_ids['test2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-11-cd76b4f606d5>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-cd76b4f606d5>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    count\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = [], []\n",
    "protein_list = ['andro','estro']\n",
    "for i in range(len(ids['training'])):\n",
    "    for prot in protein_list : \n",
    "        if np.str.find(ids['training'][i],prot) == 0:\n",
    "\n",
    "        \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.str.find(ids['training'][10],'androgen')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ids / toxicity / coords / features\n",
    "def task_split(dataset_name, protein_list) :\n",
    "    x1, x2 = [], []\n",
    "    for i, in range(len(ids[dataset_name])):\n",
    "        for prot in protein_list: \n",
    "            if\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges = []\n",
    "for feature_data in features['training']:\n",
    "    charges.append(feature_data[..., columns['partialcharge']])\n",
    "\n",
    "charges = np.concatenate([c.flatten() for c in charges])\n",
    "\n",
    "m = charges.mean()\n",
    "std = charges.std()\n",
    "print('charges: mean=%s, sd=%s' % (m, std))\n",
    "print('use sd as scaling factor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset_name, indices, rotation=0):\n",
    "    global coords, features, std\n",
    "    x = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        coords_idx = rotate(coords[dataset_name][idx], rotation)\n",
    "        features_idx = features[dataset_name][idx]\n",
    "        x.append(make_grid(coords_idx, features_idx,\n",
    "                 grid_resolution=1.0,\n",
    "                 max_dist=10.0))\n",
    "    x = np.vstack(x)\n",
    "    x[..., columns['partialcharge']] /= std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n---- DATA ----\\n')\n",
    "\n",
    "tmp = get_batch('training', range(min(50, len(features['training']))))\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ((tmp[:, :, :, :, columns['molcode']] == 0.0).any()\n",
    "        and (tmp[:, :, :, :, columns['molcode']] == 1.0).any()\n",
    "        and (tmp[:, :, :, :, columns['molcode']] == -1.0).any()).all()\n",
    "\n",
    "idx1 = [[i[0]] for i in np.where(tmp[:, :, :, :, columns['molcode']] == 1.0)]\n",
    "idx2 = [[i[0]] for i in np.where(tmp[:, :, :, :, columns['molcode']] == -1.0)]\n",
    "\n",
    "print('\\nexamples:')\n",
    "for mtype, mol in [['ligand', tmp[idx1]], ['protein', tmp[idx2]]]:\n",
    "    print(' ', mtype)\n",
    "    for name, num in columns.items():\n",
    "        print('  ', name, mol[0, num])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_baseline = ((toxicity['training'] - toxicity['training'].mean()) ** 2.0).mean()\n",
    "v_baseline = ((toxicity['validation'] - toxicity['training'].mean()) ** 2.0).mean()\n",
    "print('baseline mse: training=%s, validation=%s' % (t_baseline, v_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NET PARAMS\n",
    "\n",
    "ds_sizes = {dataset: len(toxicity[dataset]) for dataset in datasets}\n",
    "_, isize, *_, in_chnls = get_batch('training', [0]).shape\n",
    "osize = 1\n",
    "\n",
    "for set_name, set_size in ds_sizes.items():\n",
    "    print('%s %s samples' % (set_size, set_name))\n",
    "\n",
    "num_batches = {dataset: ceil(size / batch_size)\n",
    "               for dataset, size in ds_sizes.items()}\n",
    "\n",
    "print(num_batches)\n",
    "# == ... == # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sizes['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = net.make_SB_network(isize=isize, in_chnls=in_chnls, osize=osize,\n",
    "                                  conv_patch=5,\n",
    "                                  pool_patch=2,\n",
    "                                  conv_channels=[64, 128, 256],\n",
    "                                  dense_sizes=[1000, 500, 200],\n",
    "                                  lmbda=0.001,\n",
    "                                  learning_rate=1e-5)\n",
    "\n",
    "#train_writer = tf.summary.FileWriter(os.path.join(logdir, 'training_set'),\n",
    "#                                     graph, flush_secs=1)\n",
    "#val_writer = tf.summary.FileWriter(os.path.join(logdir, 'validation_set'),\n",
    "#                                   flush_secs=1)\n",
    "\n",
    "net_summaries, training_summaries = net.make_summaries_SB(graph)\n",
    "\n",
    "x = graph.get_tensor_by_name('input/structure:0')\n",
    "y = graph.get_tensor_by_name('output/prediction:0')\n",
    "t = graph.get_tensor_by_name('input/toxicity:0')\n",
    "keep_prob = graph.get_tensor_by_name('fully_connected/keep_prob:0')\n",
    "train = graph.get_tensor_by_name('training/train:0')  \n",
    "\n",
    "#train1 = graph.get_tensor_by_name('training/train1:0')  \n",
    "#train2 = graph.get_tensor_by_name('training/train2:0')  \n",
    "# graph.get_tensor_by_name => bring tensors from a certain variable scope by using name\n",
    "# this code is in the vriable_scope('training') in net_3.py \n",
    "#    >>> train = optimizer.minimize(cost, global_step=global_step,name='train')\n",
    "#\n",
    "mse = graph.get_tensor_by_name('training/mse:0')\n",
    "#mse2 = graph.get_tensor_by_name('training/mse2:0')\n",
    "feature_importance = graph.get_tensor_by_name('net_properties/feature_importance:0')\n",
    "global_step = graph.get_tensor_by_name('training/global_step:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = '_'.join((str(i) for i in conv_channels))\n",
    "fcs = '_'.join((str(i) for i in dense_sizes))\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep=to_keep)\n",
    "\n",
    "\n",
    "    \n",
    "def batches(set_name):\n",
    "    \"\"\"Batch generator, yields slice indices\"\"\"\n",
    "    global num_batches, args, ds_sizes \n",
    "    # num_batches = how many batches in each dataset(train, valid, test)\n",
    "    # ds_sizes = dataset_sizes \n",
    "    for b in range(num_batches[set_name]):\n",
    "        bi = b * batch_size # one batch mul batch_size \n",
    "        bj = (b + 1) * batch_size \n",
    "        if b == num_batches[set_name] - 1:\n",
    "            bj = ds_sizes[set_name] # maybe only remainer set\n",
    "        yield bi, bj\n",
    "\n",
    "err = float('inf')\n",
    "\n",
    "train_sample = min(batch_size, len(features['training']))\n",
    "val_sample = min(batch_size, len(features['validation']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_split(dataset_name, indices, rotation=0):\n",
    "    global coords, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = float('inf')\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pafnucy_env",
   "language": "python",
   "name": "pafnuvy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
